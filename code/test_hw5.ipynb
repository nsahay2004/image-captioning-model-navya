{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5: Image Captioning Tests\n",
    "---\n",
    "\n",
    "This is the Test Notebook that goes with **Homework 5: Image Captioning**! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import *\n",
    "from decoder import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test RNN Decoder\n",
    "# DO NOT CHANGE\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "input_size = 4\n",
    "window_size = 16\n",
    "embed_size = 8\n",
    "vocab_size = 2\n",
    "\n",
    "SOLUTION_SHAPE = (input_size, window_size, vocab_size)\n",
    "\n",
    "hidden_size = 32\n",
    "\n",
    "rnn_decoder = RNNDecoder(vocab_size, hidden_size, window_size)\n",
    "out = rnn_decoder(\n",
    "    tf.random.uniform([input_size, 64], minval=0, maxval=vocab_size),\n",
    "    tf.random.uniform([input_size, window_size], maxval=vocab_size)\n",
    ")\n",
    "epsilon = 1e-5\n",
    "assert out.shape == SOLUTION_SHAPE, \"Incorrect output shape\"\n",
    "assert not (tf.reduce_sum(out) > input_size * window_size - epsilon and tf.reduce_sum(out) < input_size * window_size + epsilon), \"Return logits, not probabilities\"\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test Transformer Decoder\n",
    "# DO NOT CHANGE\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "input_size = 4\n",
    "window_size = 16\n",
    "embed_size = 8\n",
    "vocab_size = 2\n",
    "\n",
    "SOLUTION_SHAPE = (input_size, window_size, vocab_size)\n",
    "\n",
    "hidden_size = 32\n",
    "\n",
    "transformer_decoder = TransformerDecoder(vocab_size, hidden_size, window_size)\n",
    "out = transformer_decoder(\n",
    "    tf.random.uniform([input_size, 64], minval=0, maxval=vocab_size),\n",
    "    tf.random.uniform([input_size, window_size], maxval=vocab_size)\n",
    ")\n",
    "epsilon = 1e-5\n",
    "assert out.shape == SOLUTION_SHAPE, \"Incorrect output shape\"\n",
    "assert not (tf.reduce_sum(out) > input_size * window_size - epsilon and tf.reduce_sum(out) < input_size * window_size + epsilon), \"Return logits, not probabilities\"\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test AttentionHead call function\n",
    "# DO NOT CHANGE\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "SOLUTION = np.array([\n",
    " [[1.1887338,  0.92231,    1.1832824,  1.4413241,  0.9284028,  1.2374643,\n",
    "   1.2092555,  1.6531762, ],\n",
    "  [1.1876637,  0.920028,   1.1839285,  1.4362004,  0.92912036, 1.2364106,\n",
    "   1.208142,   1.6493324, ],\n",
    "  [1.1947478,  0.92511594, 1.18963,   1.4394116,  0.9363934,  1.2449214,\n",
    "   1.2101097,  1.6523575, ],\n",
    "  [1.177351,   0.9136951,  1.1745526,  1.4348177,  0.91724336, 1.2238563,\n",
    "   1.2058568,  1.6473322, ]],\n",
    "\n",
    " [[1.171719,   0.91546506, 1.2353648,  1.2881929,  0.955095,   1.2792217,\n",
    "   1.3040287,  1.6174344, ],\n",
    "  [1.1937302,  0.9304202,  1.2462558,  1.3073134,  0.9750139,  1.3008424,\n",
    "   1.2973801,  1.6244173, ],\n",
    "  [1.1734747,  0.9167778,  1.2366297,  1.2896433,  0.9566818,  1.281148,\n",
    "   1.304501,   1.618589,  ],\n",
    "  [1.204712,   0.9370769,  1.2490294,  1.3173547,  0.9849655,  1.3102754,\n",
    "   1.2873328,  1.6238908, ]]])\n",
    "\n",
    "def create_deterministic_attention_head(input_size, output_size, is_self_attention):\n",
    "    head = AttentionHead(input_size, output_size, is_self_attention)\n",
    "\n",
    "    head.K = tf.random.uniform([input_size, output_size]) # This is not correct. This is for testing purposes only\n",
    "    head.V = tf.random.uniform([input_size, output_size]) # This is not correct. This is for testing purposes only\n",
    "    head.Q = tf.random.uniform([input_size, output_size]) # This is not correct. This is for testing purposes only\n",
    "\n",
    "    return head\n",
    "\n",
    "input_size = 4\n",
    "batch_size = 2\n",
    "window_size_keys = 3\n",
    "window_size_values = 3\n",
    "window_size_queries = 4\n",
    "\n",
    "head = create_deterministic_attention_head(4, 8, False)\n",
    "\n",
    "out = head.call(\n",
    "    tf.random.uniform([batch_size, window_size_keys, input_size], dtype=np.float32),\n",
    "    tf.random.uniform([batch_size, window_size_values, input_size], dtype=np.float32),\n",
    "    tf.random.uniform([batch_size, window_size_queries, input_size], dtype=np.float32)\n",
    ").numpy()\n",
    "\n",
    "assert out.shape == SOLUTION.shape, \"Incorrect output shape\"\n",
    "try: assert np.allclose(out, SOLUTION, rtol=0.001), \"Incorrect output values\"\n",
    "except AssertionError:\n",
    "    print(\"Output:\")\n",
    "    print(out)\n",
    "    print(\"Solution:\")\n",
    "    print(SOLUTION)\n",
    "    assert False, \"Incorrect output values\"\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_deterministic_attention_head' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m multiheadedattention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     31\u001b[0m transformer_block \u001b[38;5;241m=\u001b[39m TransformerBlock(embedding_size, multiheadedattention)\n\u001b[0;32m---> 32\u001b[0m transformer_block\u001b[38;5;241m.\u001b[39mself_atten \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_deterministic_attention_head\u001b[49m(embedding_size, embedding_size, \u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# This is not correct. This is for testing purposes only\u001b[39;00m\n\u001b[1;32m     33\u001b[0m transformer_block\u001b[38;5;241m.\u001b[39mself_context_atten \u001b[38;5;241m=\u001b[39m create_deterministic_attention_head(embedding_size, embedding_size, \u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# This is not correct. This is for testing purposes only\u001b[39;00m\n\u001b[1;32m     34\u001b[0m transformer_block\u001b[38;5;241m.\u001b[39mff_layer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m, kernel_initializer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39minitializers\u001b[38;5;241m.\u001b[39mconstant()) \u001b[38;5;66;03m#  # This is not correct. This is for testing purposes only\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_deterministic_attention_head' is not defined"
     ]
    }
   ],
   "source": [
    "# Test TransformerBlock\n",
    "# DO NOT CHANGE\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "SOLUTION = np.array([\n",
    " [[0.         ,0.6974105  ,0.98577017 ,0.        ],\n",
    "  [0.         ,0.31697747 ,1.4552947  ,0.        ],\n",
    "  [0.         ,0.         ,1.6302229  ,0.        ],\n",
    "  [0.8908619  ,0.761421   ,0.         ,0.        ],\n",
    "  [0.         ,1.093846   ,0.         ,0.85964626],\n",
    "  [0.         ,0.         ,1.1071919  ,0.84442014],\n",
    "  [0.         ,0.7972666  ,0.         ,1.1266519 ],\n",
    "  [0.         ,0.         ,1.6696594  ,0.        ]],\n",
    "\n",
    " [[0.         ,0.         ,1.6952605  ,0.        ],\n",
    "  [0.         ,1.040991   ,0.9518903  ,0.        ],\n",
    "  [0.         ,0.9950613  ,0.19130549 ,0.47112876],\n",
    "  [0.         ,1.2255263  ,0.72682804 ,0.        ],\n",
    "  [0.59422725 ,0.         ,1.3293806  ,0.        ],\n",
    "  [1.56447    ,0.         ,0.         ,0.01488148],\n",
    "  [0.         ,0.6034657  ,0.         ,1.1571136 ],\n",
    "  [0.         ,0.18686305 ,0.29973274 ,1.1265295 ]]])\n",
    "\n",
    "batch_size = 2\n",
    "input_seq_length = 8\n",
    "embedding_size = 4\n",
    "context_seq_length = 6\n",
    "multiheadedattention = False\n",
    "\n",
    "transformer_block = TransformerBlock(embedding_size, multiheadedattention)\n",
    "transformer_block.self_atten = create_deterministic_attention_head(embedding_size, embedding_size, True)  # This is not correct. This is for testing purposes only\n",
    "transformer_block.self_context_atten = create_deterministic_attention_head(embedding_size, embedding_size, True)  # This is not correct. This is for testing purposes only\n",
    "transformer_block.ff_layer = tf.keras.layers.Dense(1, kernel_initializer=tf.keras.initializers.constant()) #  # This is not correct. This is for testing purposes only\n",
    "\n",
    "out = transformer_block(\n",
    "    tf.random.uniform([batch_size, input_seq_length, embedding_size]),\n",
    "    tf.random.uniform([batch_size, context_seq_length, embedding_size])\n",
    ").numpy()\n",
    "\n",
    "assert out.shape == SOLUTION.shape, \"Incorrect output shape\"\n",
    "assert not (out < 0).any(), \"Incorrect output activation\"\n",
    "try: assert np.allclose(out, SOLUTION, rtol=0.001), \"Incorrect output values\"\n",
    "except AssertionError:\n",
    "    print(\"Output:\")\n",
    "    print(out)\n",
    "    print(\"Solution:\")\n",
    "    print(SOLUTION)\n",
    "    assert False, \"Incorrect output values\"\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test Positional Encoding\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "SOLUTION = np.array([\n",
    " [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
    "   1.0000000e+00,  1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
    " [ 8.4147096e-01,  9.9833414e-02,  9.9998331e-03,  9.9999981e-04,\n",
    "   5.4030228e-01,  9.9500418e-01,  9.9994999e-01,  9.9999952e-01],\n",
    " [ 9.0929741e-01,  1.9866933e-01,  1.9998666e-02,  1.9999987e-03,\n",
    "  -4.1614684e-01,  9.8006660e-01,  9.9980003e-01,  9.9999797e-01],\n",
    " [ 1.4112000e-01,  2.9552022e-01,  2.9995501e-02,  2.9999956e-03,\n",
    "  -9.8999250e-01,  9.5533651e-01,  9.9955004e-01,  9.9999553e-01]])\n",
    "\n",
    "input_size = 4\n",
    "window_size = 4\n",
    "embed_size = 8\n",
    "vocab_size = 2\n",
    "\n",
    "dummy_layer = tf.keras.layers.Dense(embed_size, kernel_initializer=tf.keras.initializers.constant())\n",
    "\n",
    "positional_encoding = PositionalEncoding(vocab_size, embed_size, window_size)\n",
    "positional_encoding.embedding = dummy_layer  # This is not correct. This is for testing purposes only\n",
    "\n",
    "out = positional_encoding.call(tf.random.uniform([input_size, window_size])).numpy()\n",
    "\n",
    "assert out.shape == SOLUTION.shape, \"Incorrect output shape\"\n",
    "try: assert np.allclose(out, SOLUTION, rtol=0.001), \"Incorrect output values\"\n",
    "except AssertionError:\n",
    "    print(\"Output:\")\n",
    "    print(out)\n",
    "    print(\"Solution:\")\n",
    "    print(SOLUTION)\n",
    "    assert False, \"Incorrect output values\"\n",
    "print(\"Tests passed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci1470",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
